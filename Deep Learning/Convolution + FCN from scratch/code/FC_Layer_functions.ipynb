{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(no_of_layers, input_dim, neurons_per_layer):\n",
    "    '''\n",
    "    no_of_layers: count\n",
    "    input_dim: m\n",
    "    neurons_per_layer: list in order L1, L2, L3 ... Lout\n",
    "    \n",
    "    returns:\n",
    "    net:    dict instance\n",
    "    '''\n",
    "    net = {\n",
    "        \"no_of_layers\": no_of_layers,\n",
    "        \"W1\": np.random.randn(neurons_per_layer[0], input_dim)*0.01,\n",
    "        \"b1\": np.zeros((neurons_per_layer[0], 1))\n",
    "    }\n",
    "    \n",
    "    for i in range(1, no_of_layers):\n",
    "        net[\"W\"+str(i+1)] = np.random.randn(neurons_per_layer[i], neurons_per_layer[i-1])*0.01\n",
    "        net[\"b\"+str(i+1)] = np.zeros((neurons_per_layer[i], 1))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (x):\n",
    "    '''\n",
    "    Parameters:\n",
    "    x - input\n",
    "    \n",
    "    Returns:\n",
    "    answer - The sigmoid vaue of 'x'\n",
    "    '''\n",
    "    answer = 1/(1 + np.exp(-x))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \n",
    "    \n",
    "    return np.maximum(np.zeros((x.shape[0], x.shape[1], x.shape[2], x.shape[3])), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = []\n",
    "    exp_Z = []\n",
    "    for i in range(0, Z.shape[0]):\n",
    "        exp_Z.append(np.exp(Z[i, :]))\n",
    "    \n",
    "    exp_Z = np.array(exp_Z)\n",
    "    sum_exp_Z = np.sum(exp_Z, axis = 0, keepdims = True)\n",
    "    \n",
    "    for i in range(0, Z.shape[0]):\n",
    "        ans = (exp_Z[i, :] / sum_exp_Z)\n",
    "        A.append(ans)\n",
    "    \n",
    "    A = np.array(A)\n",
    "    A = A.reshape((A.shape[0], A.shape[2]))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  forwardPropagation (A_prev, W, b, activation):\n",
    "    '''\n",
    "    Parameters:\n",
    "    model = perceptron class instance, where it is the model initialized using initializeParameters\n",
    "    X =     [[all dim1 vals], [all dim2 vals]]    i.e.inputs stacked vertically\n",
    "\n",
    "    Returns:\n",
    "    modelOutput = The class instance with 'x', 'Z1', 'A1' and 'Z2', 'A2' in it\n",
    "    '''\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if(activation == \"sigmoid\"):\n",
    "        A = sigmoid(Z)\n",
    "    elif(activation == \"softmax\"):\n",
    "        A = softmax(Z)\n",
    "    else:\n",
    "        print(\"wrong activation\")\n",
    "        \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(net, X):\n",
    "    '''\n",
    "    net:   \n",
    "    X:     \n",
    "    \n",
    "    return:\n",
    "    net:       \n",
    "    '''\n",
    "    no_of_layers = net[\"no_of_layers\"]\n",
    "    net[\"X\"] = X\n",
    "    \n",
    "    A_prev = X\n",
    "    activation = \"sigmoid\"\n",
    "    \n",
    "    for i in range(1, no_of_layers):\n",
    "        W = net[\"W\"+str(i)]\n",
    "        b = net[\"b\"+str(i)]\n",
    "        net[\"AL\"+str(i)], net[\"ZL\"+str(i)] = forwardPropagation (A_prev, W, b, activation)\n",
    "        A_prev = net[\"AL\"+str(i)]\n",
    "    \n",
    "    W = net[\"W\"+str(no_of_layers)]\n",
    "    b = net[\"b\"+str(no_of_layers)]\n",
    "    net[\"AL\"+str(no_of_layers)], net[\"ZL\"+str(no_of_layers)] = forwardPropagation (A_prev, W, b, activation = \"softmax\")\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostCalculation(m, net_out, Y):\n",
    "    '''\n",
    "    net_out:  \n",
    "    Y: ground truth\n",
    "    \n",
    "    reutrn:\n",
    "    cost\n",
    "    '''\n",
    "    \n",
    "    # one hot encode\n",
    "    encoded_Y = to_categorical(Y)\n",
    "    encoded_Y = encoded_Y.reshape(encoded_Y.shape[1], encoded_Y.shape[2])\n",
    "    encoded_Y = encoded_Y.T\n",
    "    \n",
    "    cost = (1/(2*m)) * np.sum(np.power(abs(encoded_Y - net_out), 2), axis = 1, keepdims = True)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackProp(net, Y):\n",
    "    '''\n",
    "    parms:\n",
    "    net =  dict\n",
    "    Y =    GT\n",
    "    \n",
    "    returns:\n",
    "    gradients =   \n",
    "    '''\n",
    "    gradients = {}\n",
    "    no_of_layers = net[\"no_of_layers\"]\n",
    "    \n",
    "    # one hot encode\n",
    "    encoded_Y = to_categorical(Y)\n",
    "    encoded_Y = encoded_Y.reshape(encoded_Y.shape[1], encoded_Y.shape[2])\n",
    "    encoded_Y = encoded_Y.T\n",
    "    m = encoded_Y.shape[1]\n",
    "    W = net[\"W\"+str(no_of_layers)]\n",
    "    b = net[\"b\"+str(no_of_layers)]\n",
    "    Z = net[\"ZL\"+str(no_of_layers)]\n",
    "    A = net[\"AL\"+str(no_of_layers)]\n",
    "    \n",
    "    dZ = (A - encoded_Y) * (A*(1-A))\n",
    "    \n",
    "    dW = (1/m)*(np.dot(dZ, net[\"AL\"+str(no_of_layers-1)].T))\n",
    "    db = (1/m)*(np.sum(dZ, axis = 1, keepdims = True))\n",
    "    gradients[\"dW\"+str(no_of_layers)] = dW\n",
    "    gradients[\"db\"+str(no_of_layers)] = db\n",
    "    \n",
    "    for i in range(1, no_of_layers-1):\n",
    "        dZ = np.dot(net[\"W\"+str(no_of_layers -i+1)].T, dZ) * (net[\"AL\"+str(no_of_layers -i)] - (net[\"AL\"+str(no_of_layers -i)]*net[\"AL\"+str(no_of_layers -i)]))\n",
    "        dW = (1/m)*(np.dot(dZ, net[\"AL\"+str(no_of_layers -i-1)].T))\n",
    "        db = (1/m)*(np.sum(dZ, axis = 1, keepdims = True))\n",
    "        gradients[\"dW\"+str(no_of_layers -i)] = dW\n",
    "        gradients[\"db\"+str(no_of_layers -i)] = db\n",
    "    \n",
    "    i = no_of_layers - 1    \n",
    "    dZ = np.dot(net[\"W\"+str(no_of_layers -i+1)].T, dZ) * (net[\"AL\"+str(no_of_layers -i)] - (net[\"AL\"+str(no_of_layers -i)]*net[\"AL\"+str(no_of_layers -i)]))\n",
    "    dW = (1/m)*(np.dot(dZ, net[\"X\"].T))\n",
    "    db = (1/m)*(np.sum(dZ, axis = 1, keepdims = True))\n",
    "    gradients[\"dW\"+str(no_of_layers -i)] = dW\n",
    "    gradients[\"db\"+str(no_of_layers -i)] = db\n",
    "    \n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  weightUpdate (net, gradients, lr_rate):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    no_of_layers = net[\"no_of_layers\"]\n",
    "    \n",
    "    for i in range(1, no_of_layers+1):\n",
    "        net[\"W\"+str(i)] = net[\"W\"+str(i)] - (lr_rate * gradients[\"dW\"+str(i)])\n",
    "        net[\"b\"+str(i)] = net[\"b\"+str(i)] - (lr_rate * gradients[\"db\"+str(i)])\n",
    "        \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Model(net, train_X, train_Y, test_X, test_Y, numberofEpochs = 100, lr_rate = 0.1):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #after flatten\n",
    "    no_of_layers = net[\"no_of_layers\"]\n",
    "    All_Train_Loss = []\n",
    "    All_Test_Loss = []\n",
    "    \n",
    "    m = train_X.shape[1]\n",
    "    \n",
    "    \n",
    "    for i in range(1, numberofEpochs+1):\n",
    "        trn_net = feedForward(net, train_X)\n",
    "        trn_loss = CostCalculation(train_X.shape[1], trn_net[\"AL\"+str(no_of_layers)], train_Y)\n",
    "        All_Train_Loss.append((1/10)*np.sum(trn_loss, axis = 0, keepdims = True))\n",
    "        \n",
    "        trn_grads = BackProp(trn_net, train_Y)\n",
    "        net = weightUpdate(trn_net, trn_grads, lr_rate)    \n",
    "        \n",
    "        tst_net = feedForward(net, test_X)\n",
    "        tst_loss = CostCalculation(test_X.shape[1], tst_net[\"AL\"+str(no_of_layers)], test_Y)\n",
    "        All_Test_Loss.append((1/10)*np.sum(tst_loss, axis = 0, keepdims = True))\n",
    "        \n",
    "        if((i%50 == 0) or (i in range(1, 10+1))):\n",
    "            print(\"epoch #\"+ str(i) + \": \\tTrain Loss = \"+ str(All_Train_Loss[-1]) + \"\\t\\t Validation Loss = \" + str(All_Test_Loss[-1]))\n",
    "    \n",
    "    out = {\n",
    "        \"net\": net,\n",
    "        \"All_Train_Loss\": All_Train_Loss,\n",
    "        \"All_Test_Loss\": All_Test_Loss\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(net, train_X, train_Y, test_X, test_Y, filters, numberofEpochs = 100, lr_rate = 0.1):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Covolving train_X+test_X\n",
    "    #on train set\n",
    "    if os.path.isfile(\"conv_all_train_X.npz\"): \n",
    "        npzfile = np.load(\"conv_all_train_X.npz\")\n",
    "        train_X_conv = npzfile['out']\n",
    "    else:\n",
    "        train_X_conv = conv_forward_all(train_X, filters)\n",
    "        np.savez(\"conv_all_train_X.npz\", out = train_X_conv)\n",
    "\n",
    "    #on test set\n",
    "\n",
    "    if os.path.isfile(\"conv_all_test_X.npz\"): \n",
    "        npzfile = np.load(\"conv_all_test_X.npz\")\n",
    "        test_X_conv = npzfile['out']\n",
    "    else:\n",
    "        test_X_conv = conv_forward_all(test_X, filters)\n",
    "        np.savez(\"conv_all_test_X.npz\", out = test_X_conv)\n",
    "    \n",
    "    #pool train_X+test_X\n",
    "    #on train set\n",
    "    if os.path.isfile(\"pool_all_train_X.npz\"): \n",
    "        npzfile = np.load(\"pool_all_train_X.npz\")\n",
    "        train_X_conv_pool = npzfile['out']\n",
    "    else:\n",
    "        train_X_conv_pool = pool_forward_all(train_X_conv)\n",
    "        np.savez(\"pool_all_train_X.npz\", out = train_X_conv_pool)\n",
    "\n",
    "    #on test set\n",
    "    if os.path.isfile(\"pool_all_test_X.npz\"): \n",
    "        npzfile = np.load(\"pool_all_test_X.npz\")\n",
    "        test_X_conv_pool = npzfile['out']\n",
    "    else:\n",
    "        test_X_conv_pool = pool_forward_all(test_X_conv)\n",
    "        np.savez(\"pool_all_test_X.npz\", out = test_X_conv_pool)\n",
    "    \n",
    "    #Apply ReLU\n",
    "    #train\n",
    "    train_X_conv_pool_relu = ReLU(train_X_conv_pool)\n",
    "    \n",
    "    #test\n",
    "    test_X_conv_pool_relu = ReLU(test_X_conv_pool)\n",
    "    \n",
    "    #flatten train_X+test_X\n",
    "    train_X_conv_pool_flatten = train_X_conv_pool.reshape(train_X_conv_pool.shape[0], -1).T\n",
    "    test_X_conv_pool_flatten = test_X_conv_pool.reshape(test_X_conv_pool.shape[0], -1).T\n",
    "    \n",
    "    #after flatten\n",
    "    no_of_layers = net[\"no_of_layers\"]\n",
    "    All_Train_Loss = []\n",
    "    All_Test_Loss = []\n",
    "    \n",
    "    m = train_Y.shape[1]\n",
    "    \n",
    "    \n",
    "    for i in range(1, numberofEpochs+1):\n",
    "        trn_net = feedForward(net, train_X_conv_pool_flatten)\n",
    "        trn_loss = CostCalculation(train_X_conv_pool_flatten.shape[1], trn_net[\"AL\"+str(no_of_layers)], train_Y)\n",
    "        All_Train_Loss.append((1/10)*np.sum(trn_loss, axis = 0, keepdims = True))\n",
    "        \n",
    "        trn_grads = BackProp(trn_net, train_Y)\n",
    "        net = weightUpdate(trn_net, trn_grads, lr_rate)    \n",
    "        \n",
    "        #below was expected tobe done in test() function\n",
    "        tst_net = feedForward(net, test_X)\n",
    "        tst_loss = CostCalculation(test_X.shape[1], tst_net[\"AL\"+str(no_of_layers)], test_Y)\n",
    "        All_Test_Loss.append((1/10)*np.sum(tst_loss, axis = 0, keepdims = True))\n",
    "        \n",
    "        if((i%50 == 0) or (i in range(1, 10+1))):\n",
    "            print(\"epoch #\"+ str(i) + \": \\tTrain Loss = \"+ str(All_Train_Loss[-1]) + \"\\t\\t Validation Loss = \" + str(All_Test_Loss[-1]))\n",
    "    \n",
    "    out = {\n",
    "        \"net\": net,\n",
    "        \"All_Train_Loss\": All_Train_Loss,\n",
    "        \"All_Test_Loss\": All_Test_Loss\n",
    "    }\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
